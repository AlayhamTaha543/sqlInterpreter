package com.sqlcompiler.compiler;

import org.antlr.v4.runtime.*;
// import org.antlr.v4.runtime.atn.PredictionMode;
import com.sqlcompiler.parser.SQLLexer;
import com.sqlcompiler.parser.SQLTokenTypeMapper;
import java.util.Scanner;

/**
 * IMPORTANT: Before running this class, execute these Maven commands:
 * 1. mvn clean generate-sources
 * 2. mvn compile
 * 
 * This will generate the SQLLexer.java file from SQLLexer.g4
 * Generated files location: target/generated-sources/antlr4/
 */

/**
 * SQL Lexer Testing Application
 * Tests the SQL lexer grammar by tokenizing SQL input
 */
public class SQLLexerApp {

    public static void main(String[] args) {
        System.out.println("╔════════════════════════════════════════════════════════════════╗");
        System.out.println("║          SQL LEXER TOKENIZATION TESTER                         ║");
        System.out.println("║          Using ANTLR4 Version 4.13.2                          ║");
        System.out.println("╚════════════════════════════════════════════════════════════════╝\n");

        // Option 1: Interactive mode
        System.out.println("Choose input mode:");
        System.out.println("1. Interactive mode (type SQL queries)");
        System.out.println("2. Test predefined SQL queries");
        System.out.print("\nEnter choice (1 or 2): ");

        Scanner scanner = new Scanner(System.in);
        String choice = scanner.nextLine().trim();

        if (choice.equals("1")) {
            interactiveMode(scanner);
        } else if (choice.equals("2")) {
            testPredefinedQueries();
        } else {
            System.out.println("Invalid choice. Running test mode by default...\n");
            testPredefinedQueries();
        }

        scanner.close();
    }

    /**
     * Interactive mode - allows user to input SQL queries one by one
     * 
     * In this mode, users can:
     * 1. Enter any SQL query they want to test
     * 2. See the tokens generated by the lexer immediately
     * 3. Type 'exit' to quit the application
     * 
     * This is useful for testing specific SQL queries or debugging the lexer
     * 
     * @param scanner Scanner object to read user input from console
     */
    private static void interactiveMode(Scanner scanner) {
        System.out.println("\n" + "=".repeat(60));
        System.out.println("INTERACTIVE MODE - Enter SQL queries to tokenize");
        System.out.println("Type 'exit' to quit");
        System.out.println("=".repeat(60) + "\n");

        while (true) {
            System.out.print("\nSQL> ");
            String input = scanner.nextLine().trim();

            if (input.equalsIgnoreCase("exit")) {
                System.out.println("\nExiting lexer tester. Goodbye!");
                break;
            }

            if (input.isEmpty()) {
                System.out.println("Please enter a SQL query.");
                continue;
            }

            tokenizeSQLQuery(input);
        }
    }

    /**
     * Test predefined SQL queries
     * 
     * This method runs 10 sample SQL queries through the lexer to demonstrate
     * that the lexer correctly tokenizes various SQL statement types:
     * - SELECT queries with WHERE, GROUP BY, ORDER BY, LIMIT clauses
     * - INSERT statements with column lists and values
     * - UPDATE statements with SET clauses
     * - DELETE statements
     * - CREATE TABLE statements
     * - JOIN operations with different join types
     * 
     * This is useful for:
     * 1. Verifying that the lexer grammar works for common SQL patterns
     * 2. Demonstrating the capabilities of the lexer
     * 3. Regression testing to ensure changes don't break existing functionality
     */
    private static void testPredefinedQueries() {
        // Sample SQL queries covering different SQL statement types
        String[] testQueries = {
            "SELECT * FROM employees",
            "SELECT id, name, salary FROM employees WHERE department = 'IT'",
            "SELECT name, AVG(salary) FROM employees GROUP BY department HAVING AVG(salary) > 50000",
            "INSERT INTO users (id, name, email) VALUES (1, 'John Doe', 'john@example.com')",
            "UPDATE employees SET salary = 65000 WHERE id = 5",
            "DELETE FROM employees WHERE department = 'HR'",
            "CREATE TABLE employees (id INT, name VARCHAR(100), salary FLOAT)",
            "SELECT * FROM orders WHERE date BETWEEN '2024-01-01' AND '2024-12-31'",
            "SELECT e.name, d.department FROM employees e JOIN departments d ON e.dept_id = d.id",
            "SELECT DISTINCT country FROM customers ORDER BY country ASC LIMIT 10"
        };

        System.out.println("\n" + "=".repeat(60));
        System.out.println("TEST MODE - Predefined SQL Queries");
        System.out.println("=".repeat(60) + "\n");

        for (int i = 0; i < testQueries.length; i++) {
            System.out.println("\n" + "-".repeat(60));
            System.out.println("Query " + (i + 1) + ":");
            System.out.println("-".repeat(60));
            tokenizeSQLQuery(testQueries[i]);
        }
    }

    /**
     * Tokenizes a SQL query and displays the tokens generated by the lexer
     * 
     * This is the core method that:
     * 1. Takes a SQL query string as input
     * 2. Creates an ANTLR CharStream from the input
     * 3. Creates an SQLLexer instance with the input stream
     * 4. Generates all tokens from the query
     * 5. Displays each token with its type, value, and position
     * 
     * Process Flow:
     *   Input SQL Query
     *        ↓
     *   CharStreams.fromString() → Creates input stream
     *        ↓
     *   SQLLexer(input) → Creates lexer from grammar
     *        ↓
     *   CommonTokenStream → Collects all tokens
     *        ↓
     *   Display results in formatted table
     * 
     * @param sqlQuery The SQL query string to tokenize
     *                 Example: "SELECT * FROM employees WHERE id = 1"
     */
    private static void tokenizeSQLQuery(String sqlQuery) {
        try {
            // Step 1: Create input stream from SQL query string
            // CharStreams provides an input stream that ANTLR can read
            CharStream input = CharStreams.fromString(sqlQuery);

            // Step 2: Create lexer instance
            // The lexer uses the SQLLexer.g4 grammar rules to recognize tokens
            SQLLexer lexer = new SQLLexer(input);

            // Step 3: Set prediction mode for better performance
            // SLL = Stronger LL prediction (faster but less powerful)
            // This is optimized for typical SQL queries
            // lexer.getInterpreter().setPredictionMode(PredictionMode.SLL);

            // Step 4: Create token stream
            // CommonTokenStream collects all tokens from the lexer
            CommonTokenStream tokens = new CommonTokenStream(lexer);

            // Step 5: Fill the token stream (force lexer to tokenize entire input)
            // This ensures all tokens are generated before we process them
            tokens.fill();

            // Display input query
            System.out.println("\nInput Query: " + sqlQuery);
            System.out.println("\nTokens Generated:");
            System.out.println(String.format("%-5s | %-20s | %-15s | %s", "Line", "Token Type", "Value", "Range"));
            System.out.println("-".repeat(70));

            int tokenCount = 0;

            // Step 6: Iterate through all tokens and display their information
            for (Token token : tokens.getTokens()) {
                // Skip EOF (End of File) token as it's not a real token
                if (token.getType() == Token.EOF) {
                    break;
                }

                // Get human-readable token type name (e.g., "SELECT" instead of "1")
                String tokenType = getTokenTypeName(lexer, token.getType());
                
                // Get the actual text of the token from the input
                String tokenValue = token.getText();
                
                // Get line number (which line in input this token appears)
                int line = token.getLine();
                
                // Get column (character position in the line)
                int column = token.getCharPositionInLine();
                
                // Calculate token length for range display
                int tokenLength = tokenValue.length();

                // Display formatted token information
                System.out.println(String.format("%-5d | %-20s | %-15s | [%d:%d-%d]",
                        line, tokenType, tokenValue, column, column, column + tokenLength));

                tokenCount++;
            }

            System.out.println("-".repeat(70));
            System.out.println("Total Tokens: " + tokenCount);

        } catch (Exception e) {
            // Handle any errors that occur during tokenization
            System.err.println("Error tokenizing query: " + e.getMessage());
            e.printStackTrace();
        }
    }

    /**
     * Gets the human-readable name of a token type
     * 
     * This method now delegates to the SQLTokenTypeMapper utility class.
     * All token type mappings are now centralized in SQLTokenTypeMapper.
     * 
     * IMPORTANT FOR DEVELOPERS:
     * When adding new tokens to the lexer grammar (SQLLexer.g4),
     * add the corresponding case statement in SQLTokenTypeMapper.getTokenName()
     * Do NOT add it here!
     * 
     * @param lexer The SQLLexer instance (no longer used, kept for backward compatibility)
     * @param tokenType The token type code from ANTLR
     * @return The human-readable token type name
     */
    private static String getTokenTypeName(SQLLexer lexer, int tokenType) {
        return SQLTokenTypeMapper.getTokenName(tokenType);
    }
}